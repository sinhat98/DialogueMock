{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s23326/Dev/DialogueMock/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "\u001b[0;93m2024-10-17 15:07:39.577010 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight, shape: {32000,768}\u001b[m\n",
      "Saving the exported ONNX model is heavily recommended to avoid having to export it again. Do so with `model.push_to_hub('colorfulscoop/sbert-base-ja', create_pr=True)`.\n",
      "\u001b[0;93m2024-10-17 15:07:39.577692 [W:onnxruntime:, coreml_execution_provider.cc:107 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 109 number of nodes in the graph: 802 number of nodes supported by CoreML: 542\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:07:40.662973 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:07:40.662981 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, export_optimized_onnx_model, export_dynamic_quantized_onnx_model\n",
    "model_id = \"colorfulscoop/sbert-base-ja\"\n",
    "output_dir = model_id.replace(\"/\", \"-\")\n",
    "onnx_model = SentenceTransformer(model_id, backend=\"onnx\", model_kwargs={\"export\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s23326/Dev/DialogueMock/.venv/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "There is no gpu for onnxruntime to do optimization.\n"
     ]
    }
   ],
   "source": [
    "for optimization_config in [\"O1\", \"O2\", \"O3\", \"O4\"]:\n",
    "    export_optimized_onnx_model(\n",
    "        onnx_model,\n",
    "        optimization_config,\n",
    "        output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quantization_config in ['arm64', 'avx2', 'avx512', 'avx512_vnni']:\n",
    "    export_dynamic_quantized_onnx_model(\n",
    "        onnx_model,\n",
    "        quantization_config,\n",
    "        output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_qint8_avx512_vnni.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n",
      "\u001b[0;93m2024-10-17 15:28:59.203091 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight_quantized, shape: {32000,768}\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:28:59.204027 [W:onnxruntime:, coreml_execution_provider.cc:107 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 109 number of nodes in the graph: 1189 number of nodes supported by CoreML: 732\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:29:00.443289 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:29:00.443300 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26436686515808105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context leak detected, msgtracer returned -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "file_name = \"onnx/model_qint8_avx512_vnni.onnx\"\n",
    "model = SentenceTransformer(output_dir, backend=\"onnx\", model_kwargs={\"file_name\": file_name})\n",
    "sentences = [\"こんにちは\", \"おはようございます\", \"おやすみなさい\", \"さようなら\"]\n",
    "tic = time.time()\n",
    "embeddings = model.encode(sentences)\n",
    "toc = time.time() - tic\n",
    "print(toc)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_O1.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n",
      "\u001b[0;93m2024-10-17 15:25:31.093703 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight, shape: {32000,768}\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:31.094400 [W:onnxruntime:, coreml_execution_provider.cc:107 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 109 number of nodes in the graph: 802 number of nodes supported by CoreML: 542\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:32.187634 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:32.187647 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "The ONNX file model_O2.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_O1.onnx, Average Time: 0.0507 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-10-17 15:25:34.205846 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight, shape: {32000,768}\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:34.206262 [W:onnxruntime:, coreml_execution_provider.cc:107 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 61 number of nodes in the graph: 493 number of nodes supported by CoreML: 209\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:34.833307 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:34.833316 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "The ONNX file model_O3.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_O2.onnx, Average Time: 0.0326 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-10-17 15:25:36.287617 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight, shape: {32000,768}\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:36.288000 [W:onnxruntime:, coreml_execution_provider.cc:107 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 61 number of nodes in the graph: 493 number of nodes supported by CoreML: 209\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:36.926905 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:36.926913 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "The ONNX file model_O4.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_O3.onnx, Average Time: 0.0427 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-10-17 15:25:38.611519 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight, shape: {32000,768}\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:38.611686 [W:onnxruntime:, coreml_execution_provider.cc:107 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 12 number of nodes in the graph: 494 number of nodes supported by CoreML: 24\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:38.688602 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight, shape: {32000,768}\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:38.750279 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:38.750287 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "The ONNX file model_qint8_avx512.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_O4.onnx, Average Time: 0.0229 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-10-17 15:25:39.873168 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight_quantized, shape: {32000,768}\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:39.874000 [W:onnxruntime:, coreml_execution_provider.cc:107 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 109 number of nodes in the graph: 1189 number of nodes supported by CoreML: 732\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:40.984518 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:40.984528 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "The ONNX file model_qint8_avx512_vnni.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: onnx/model_qint8_avx512.onnx, Average Time: 0.0297 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-10-17 15:25:42.377259 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:embeddings.word_embeddings.weight_quantized, shape: {32000,768}\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:42.378080 [W:onnxruntime:, coreml_execution_provider.cc:107 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 109 number of nodes in the graph: 1189 number of nodes supported by CoreML: 732\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:43.500035 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-10-17 15:25:43.500044 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n",
      "\u001b[1;31m2024-10-17 15:25:43.990074 [E:onnxruntime:, sequential_executor.cc:516 ExecuteKernel] Non-zero status code returned while running MatMulInteger node. Name:'/encoder/layer.1/attention/self/MatMul_1_quant' Status Message: matmul_helper.h:162 Compute MatMul dimension mismatch\u001b[m\n"
     ]
    },
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running MatMulInteger node. Name:'/encoder/layer.1/attention/self/MatMul_1_quant' Status Message: matmul_helper.h:162 Compute MatMul dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFail\u001b[0m                                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mランチって何時からでしたっけ\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 21\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m     23\u001b[0m time_list\u001b[38;5;241m.\u001b[39mappend(toc)\n",
      "File \u001b[0;32m~/Dev/DialogueMock/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:621\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 621\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    623\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/Dev/DialogueMock/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:688\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    687\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Dev/DialogueMock/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/DialogueMock/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/DialogueMock/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:350\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    348\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/Dev/DialogueMock/.venv/lib/python3.10/site-packages/optimum/modeling_base.py:98\u001b[0m, in \u001b[0;36mOptimizedModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/DialogueMock/.venv/lib/python3.10/site-packages/optimum/onnxruntime/modeling_ort.py:1107\u001b[0m, in \u001b[0;36mORTModelForFeatureExtraction.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: token_type_ids}\n\u001b[1;32m   1106\u001b[0m onnx_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_onnx_inputs(use_torch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n\u001b[0;32m-> 1107\u001b[0m onnx_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_onnx_outputs(use_torch, \u001b[38;5;241m*\u001b[39monnx_outputs)\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_names:\n",
      "File \u001b[0;32m~/Dev/DialogueMock/.venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mFail\u001b[0m: [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running MatMulInteger node. Name:'/encoder/layer.1/attention/self/MatMul_1_quant' Status Message: matmul_helper.h:162 Compute MatMul dimension mismatch"
     ]
    }
   ],
   "source": [
    "import time\n",
    "file_names = [\n",
    "    \"model_O1.onnx\",\n",
    "    \"model_O2.onnx\",\n",
    "    \"model_O3.onnx\",\n",
    "    \"model_O4.onnx\",\n",
    "    \"onnx/model_qint8_avx512.onnx\",\n",
    "    \"onnx/model_qint8_avx512_vnni.onnx\",\n",
    "    \"onnx/model_qint8_avx2.onnx\",\n",
    "    \"onnx/model_qint8_arm64.onnx\",\n",
    "]\n",
    "\n",
    "n_iter = 30\n",
    "\n",
    "for file_name in file_names:\n",
    "    model = SentenceTransformer(output_dir, backend=\"onnx\", model_kwargs={\"file_name\": file_name})\n",
    "    time_list = []\n",
    "    for i in range(n_iter):\n",
    "        sentences = [\"ランチって何時からでしたっけ\"]\n",
    "        tic = time.perf_counter()\n",
    "        embeddings = model.encode(sentences)\n",
    "        toc = time.perf_counter() - tic\n",
    "        time_list.append(toc)\n",
    "    print(f\"Model: {file_name}, Average Time: {sum(time_list) / n_iter:.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "from src.utils import get_custom_logger\n",
    "logger = get_custom_logger(\"faq\")\n",
    "\n",
    "model_dir = \"/Users/s23326/Dev/DialogueMock/colorfulscoop-sbert-base-ja/\"\n",
    "model_file = \"onnx/model_qint8_avx512_vnni.onnx\"\n",
    "\n",
    "class SbertFAQ:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # モデルをロード\n",
    "        logger.info(\"Loading ONNX model...\")\n",
    "        start_time = time.time()\n",
    "        self.model = SentenceTransformer(model_dir, backend=\"onnx\", model_kwargs={\"model_file\": model_file})\n",
    "        logger.info(f\"Model loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "        \n",
    "        # FAQデータを設定\n",
    "        self.faq_dict = {\n",
    "            \"営業時間\": [\n",
    "                \"ランチの営業時間は11:00から15:00です\",\n",
    "                \"ディナーの営業時間は17:00から23:00です\",\n",
    "                \"営業時間は日によって異なる場合がありますので、お問い合わせください\"\n",
    "            ],\n",
    "            \"駐車場\": [\n",
    "                \"駐車場は2台まで停められます\",\n",
    "                \"近隣にコインパーキングもございます\"\n",
    "            ],\n",
    "            \"席代\": [\n",
    "                \"ランチは席代がかかりませんが、ディナーは席代がかかります\",\n",
    "                \"ディナータイムの席代はお一人様500円です\"\n",
    "            ],\n",
    "            \"予約\": [\n",
    "                \"ランチは予約できません\",\n",
    "                \"ディナーのご予約はお電話で承っております\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # 質問キーワードと回答を準備\n",
    "        self.questions = list(self.faq_dict.keys())\n",
    "        self.question_embeddings = self.encode_sentences(self.questions)\n",
    "        \n",
    "        # 各質問キーワードに対応する回答をベクトル化\n",
    "        self.answers_list = []\n",
    "        self.answer_embeddings = []\n",
    "        for key in self.questions:\n",
    "            answers = self.faq_dict[key]\n",
    "            embeddings = self.encode_sentences(answers)\n",
    "            self.answers_list.append(answers)\n",
    "            self.answer_embeddings.append(embeddings)\n",
    "    \n",
    "    def encode_sentences(self, sentences):\n",
    "        # 文をエンコード（ベクトル化）\n",
    "        embeddings = self.model.encode(sentences)\n",
    "        return embeddings\n",
    "    \n",
    "    def get_answer(self, query):\n",
    "        # ユーザーの質問をエンコード\n",
    "        query_embedding = self.encode_sentences([query])[0]\n",
    "        \n",
    "        # 質問キーワードとの類似度を計算\n",
    "        similarities = np.dot(self.question_embeddings, query_embedding) / (np.linalg.norm(self.question_embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
    "        \n",
    "        max_sim = similarities.max()\n",
    "        threshold = 0.5  # 閾値を適切に設定してください\n",
    "        if max_sim > threshold:\n",
    "            best_question_idx = similarities.argmax()\n",
    "            best_question = self.questions[best_question_idx]\n",
    "            \n",
    "            # 対応する回答リスト内で最適な回答を選択\n",
    "            answer_embeddings = self.answer_embeddings[best_question_idx]\n",
    "            answer_similarities = np.dot(answer_embeddings, query_embedding) / (np.linalg.norm(answer_embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
    "            best_answer_idx = answer_similarities.argmax()\n",
    "            best_answer = self.answers_list[best_question_idx][best_answer_idx]\n",
    "            return best_answer\n",
    "        else:\n",
    "            return \"申し訳ありませんが、そのご質問にはお答えできません。\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # クラスのインスタンスを作成\n",
    "    faq = SbertFAQ()\n",
    "    \n",
    "    # テスト用の質問リスト\n",
    "    queries = [\n",
    "        \"ランチって設置代って追加でかかったりしますか\",  # 音声認識誤りを含む質問\n",
    "        \"ディナー予約できますか？\",\n",
    "        \"駐車場はありますか？\",\n",
    "        \"ランチの時間を教えて下さい\",\n",
    "    ]\n",
    "    \n",
    "    # 質問に対する回答を取得\n",
    "    for query in queries:\n",
    "        answer = faq.get_answer(query)\n",
    "        print(f\"質問: {query}\")\n",
    "        print(f\"回答:\\n{answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
